{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "12a589c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install datasets tqdm nltk numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "889d1bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikipedia (/home/gudkovv/.cache/huggingface/datasets/wikipedia/20220301.simple/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d89024b311458ea6352b9d6727545e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba27a6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'url', 'title', 'text'],\n",
       "        num_rows: 205328\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Для создания языковой модели будем работать с корпусом упрощенной английской википедии\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7619ad72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A or a is the first letter of the English alphabet. The small letter, a or α, is used as a lower case vowel. \\n\\nWhen it is spoken,  ā is said as a long a, a diphthong of ĕ and y. A is similar to  alpha of the Greek alphabet. That is not surprising, because it stands for the same sound. \\n\\n\"Alpha and omega\" (the last letter of the Greek alphabet) means from beginning to the end. In musical notation, the letter A is the symbol of a note in the scale, below B and above G. In binary numbers, the letter A is 01000001.\\n\\nA is the letter that was used to represent a team in an old TV show, The A-Team. A capital a is written \"A\". Use a capital a at the start of a sentence if writing.\\n\\nWhere it came from\\nThe letter \\'A\\' was in the Phoenician alphabet\\'s aleph. This symbol came from a simple picture of an ox head. \\n\\nThis Phoenician letter helped make the basic blocks of later types of the letter. The Greeks later modified this letter and used it as their letter alpha. The Greek alphabet was used by the Etruscans in northern Italy, and the Romans later modified the Etruscan alphabet for their own language.\\n\\nUsing the letter\\nThe letter A has six different sounds. It can sound like æ, in the International Phonetic Alphabet, such as the word pad. Other sounds of this letter are in the words father, which developed into another sound, such as in the word ace.\\n\\nUse in mathematics\\nIn algebra, the letter \"A\" along with other letters at the beginning of the alphabet is used to represent known quantities.\\n\\nIn geometry, capital A, B, C etc. are used to label line segments, lines, etc. Also, A is typically used as one of the letters to label an angle in a triangle.\\n\\nReferences\\n\\nBasic English 850 words\\nVowel letters'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['text'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91f10b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10ce2fe9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Сначала предобработаем корпус\n",
    "# Каждое предложение из корпуса будет расцениваться как отдельный текст\n",
    "# Токенизируем каждое предложение\n",
    "# Все числа заменим на токен <NUM>\n",
    "# В начале и конце каждого токенизированного предложения добавим токены <START> и <END>\n",
    "\n",
    "corpus = []\n",
    "for elem in tqdm(dataset['train'], total=dataset['train'].num_rows):\n",
    "    text = elem['text']\n",
    "    for sent in sent_tokenize(text):\n",
    "        # Токенизируем каждое предложение, добавляем необходимые токены и закидываем в corpus\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "beb85aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(tokens, n):\n",
    "    grams = []\n",
    "    return grams\n",
    "\n",
    "# Эта функция нужна для создания пар (context, next_token)\n",
    "\n",
    "# Например,\n",
    "# n_grams(['<START>', 'a', 'cat', 'sat', 'on', 'the', 'mat', '.', '<END>'], 1)\n",
    "\n",
    "# Должен превратиться в:\n",
    "# [[['<START>'], 'a'],\n",
    "#  [['a'], 'cat'],\n",
    "#  [['cat'], 'sat'],\n",
    "#  [['sat'], 'on'],\n",
    "#  [['on'], 'the'],\n",
    "#  [['the'], 'mat'],\n",
    "#  [['mat'], '.'],\n",
    "#  [['.'], '<END>']]\n",
    "\n",
    "# А n_grams(['<START>', 'a', 'cat', 'sat', 'on', 'the', 'mat', '.', '<END>'], 2)\n",
    "\n",
    "# Должен превратиться в:\n",
    "# [[['<START>', 'a'], 'cat'],\n",
    "#  [['a', 'cat'], 'sat'],\n",
    "#  [['cat', 'sat'], 'on'],\n",
    "#  [['sat', 'on'], 'the'],\n",
    "#  [['on', 'the'], 'mat'],\n",
    "#  [['the', 'mat'], '.'],\n",
    "#  [['mat', '.'], '<END>']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d199ee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Get vocabyulary\n",
    "# 2) defaultdict of each n_gram equal Counter(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4260731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 397739/397739 [00:42<00:00, 9345.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# Здесь накопим статистику\n",
    "# Сначала для простоты накопим для униграмм, затем для биграмм и так далее, по необходимости\n",
    "# Статистика должна иметь итоговый вид (можно воспользоваться Counter и defaultdict):\n",
    "\n",
    "# Если n=2 (работаем с биграммами), то статистика должна накопиться для каждого убывающего уровня\n",
    "# (в данном случае - и для униграмм)\n",
    "\n",
    "# {\n",
    "#     (n_gram): {\n",
    "#             next_word_i: count,\n",
    "#               .\n",
    "#               .\n",
    "#             },\n",
    "#       .\n",
    "#       .\n",
    "#       .\n",
    "# }\n",
    "\n",
    "stats = defaultdict(Counter)\n",
    "n = 2\n",
    "\n",
    "for sent in tqdm(corpus):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "101a3860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'from': 1,\n",
       "         '—': 1,\n",
       "         ',': 11,\n",
       "         '.': 20,\n",
       "         'in': 3,\n",
       "         'and': 3,\n",
       "         '(': 8,\n",
       "         'by': 2,\n",
       "         ':': 1,\n",
       "         'kubrick': 1,\n",
       "         'had': 1,\n",
       "         'references': 1,\n",
       "         'is': 3,\n",
       "         '-citizendium': 1,\n",
       "         'space': 2,\n",
       "         \"''\": 2,\n",
       "         'two': 1,\n",
       "         'are': 1,\n",
       "         \"'\": 1,\n",
       "         'works': 1,\n",
       "         ';': 1,\n",
       "         'combined': 2,\n",
       "         'minivan': 1,\n",
       "         'sold': 1,\n",
       "         'of': 2})"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats[tuple([\"odyssey\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4a79e106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ancient': 1})"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats[tuple([\"odyssey\", \"from\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "78b506d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Семплирование реализовано за вас. Суть: взять список слов, соответствующие словам частотности и выделить,\n",
    "# основываясь на частотности, случайным образом один из токенов\n",
    "# Это умеет делать np.random.choice(), если передать ему аргумент p - список вероятностей\n",
    "\n",
    "def sample(n_gram_stat):\n",
    "    words = list(n_gram_stat.keys())\n",
    "    pc = np.array(list(n_gram_stat.values()))\n",
    "    pc = pc/np.sum(pc)\n",
    "    return np.random.choice(words, p=pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "cc0c1e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Осталось сгенерировать последовательность\n",
    "\n",
    "# Условия генерации:\n",
    "# 1) Начинаем с токена <START>\n",
    "# 2) Берем k последних слов (2, если работаем с биграммами)\n",
    "# 3) Семплируем следующее слово из статистики\n",
    "# 4) Если слова нет в статистике, сдвигаем контекст на 1 в сторону конца\n",
    "# 5) Повторяем операцию\n",
    "# 6) Заканчиваем генерацию, если засемплили <END> или пробили сверху length по длине сгенерированной последовательности\n",
    "def generate_sequence(stats, k, length=10):\n",
    "    sentence = [\"<START>\"]\n",
    "    while len(sentence) < length:\n",
    "        pass\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f22057c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<START>',\n",
       " 'sandals',\n",
       " 'are',\n",
       " 'worn',\n",
       " 'for',\n",
       " 'many',\n",
       " 'years',\n",
       " '.',\n",
       " '<END>',\n",
       " 'city']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sequence(stats, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
